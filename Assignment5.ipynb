{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "faca5110",
      "metadata": {
        "id": "faca5110"
      },
      "source": [
        "# Assigment 5\n",
        "\n",
        "**Submission deadlines**:\n",
        "\n",
        "* last lab before 20.06.2023\n",
        "\n",
        "**Points:** Aim to get 6 (updated value) out of 15+ possible points\n",
        "\n",
        "All needed data files are on Drive: <https://drive.google.com/drive/folders/1uufpGn46Mwv4oBwajIeOj4rvAK96iaS-?usp=sharing> (or will be soon :) )\n",
        "\n",
        "## Task 1 (5 points)\n",
        "\n",
        "Consider the vowel reconstruction task -- i.e. inserting missing vowels (aeuioy) to obtain proper English text. For instance for the input sentence:\n",
        "\n",
        "<pre>\n",
        "h m gd smbd hs stln ll m vwls\n",
        "</pre>\n",
        "\n",
        "the best result is\n",
        "\n",
        "<pre>\n",
        "oh my god somebody has stolen all my vowels\n",
        "</pre>\n",
        "\n",
        "In this task both dev and test data come from the two books about Winnie-the-Pooh. You have to train two RNN Language Models on *pooh-train.txt*. For the first model use the code below, for the second choose different hyperparameters (different dropout, smaller number of units or layers, or just do any modification you want).\n",
        "\n",
        "The code below is based on\n",
        "https://www.kdnuggets.com/2020/07/pytorch-lstm-text-generation-tutorial.html"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! gdown https://drive.google.com/uc?id=1-k8e9OG7NOVk73Kkv4WpqNQKHrVVmVXa\n",
        "! gdown https://drive.google.com/uc?id=1ADNyasf6AEUsmz-163DWHw_rSldfnpta\n",
        "! gdown https://drive.google.com/uc?id=1POiC9I_BjZKBQe-7XkW5CW0z8_6inWtY\n",
        "! ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rE43jLtazcP",
        "outputId": "cdbe8f37-b17d-4a4d-d556-76e20fc0016d"
      },
      "id": "7rE43jLtazcP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-k8e9OG7NOVk73Kkv4WpqNQKHrVVmVXa\n",
            "To: /content/pooh_train.txt\n",
            "100% 255k/255k [00:00<00:00, 145MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ADNyasf6AEUsmz-163DWHw_rSldfnpta\n",
            "To: /content/pooh_test.txt\n",
            "100% 34.6k/34.6k [00:00<00:00, 115MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1POiC9I_BjZKBQe-7XkW5CW0z8_6inWtY\n",
            "To: /content/pooh_words.txt\n",
            "100% 20.4k/20.4k [00:00<00:00, 72.1MB/s]\n",
            "pooh_test.txt  pooh_train.txt  pooh_words.txt  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78d50e03",
      "metadata": {
        "id": "78d50e03"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from collections import Counter\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "SEQUENCE_LENGTH = 15\n",
        "\n",
        "class PoohDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, sequence_length, device):\n",
        "        txt = open('pooh_train.txt').read()\n",
        "\n",
        "        self.words = txt.lower().split() # The text is already tokenized\n",
        "\n",
        "        self.uniq_words = self.get_uniq_words()\n",
        "\n",
        "        self.index_to_word = {index: word for index, word in enumerate(self.uniq_words)}\n",
        "        self.word_to_index = {word: index for index, word in enumerate(self.uniq_words)}\n",
        "\n",
        "        self.words_indexes = [self.word_to_index[w] for w in self.words]\n",
        "        self.sequence_length = sequence_length\n",
        "        self.device = device\n",
        "\n",
        "\n",
        "    def get_uniq_words(self):\n",
        "        word_counts = Counter(self.words)\n",
        "        return sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.words_indexes) - self.sequence_length\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return (\n",
        "            torch.tensor(self.words_indexes[index:index+self.sequence_length], device=self.device),\n",
        "            torch.tensor(self.words_indexes[index+1:index+self.sequence_length+1], device=self.device)\n",
        "        )\n",
        "\n",
        "pooh_dataset = PoohDataset(SEQUENCE_LENGTH, device)\n",
        "test_dataset = open('pooh_test.txt').read()\n",
        "test_dataset = test_dataset.lower().split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b22a87b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b22a87b",
        "outputId": "5dee7ad3-317f-486e-833a-d4fec9bedf15"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMModel(\n",
              "  (embedding): Embedding(2548, 100)\n",
              "  (lstm): LSTM(100, 512, num_layers=2, dropout=0.2)\n",
              "  (fc): Linear(in_features=512, out_features=2548, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "from torch import nn, optim\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, dataset, device):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.lstm_size = 512\n",
        "        self.embedding_dim = 100\n",
        "        self.num_layers = 2\n",
        "        self.device = device\n",
        "\n",
        "\n",
        "        n_vocab = len(dataset.uniq_words)\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=n_vocab,\n",
        "            embedding_dim=self.embedding_dim,\n",
        "        )\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=self.embedding_dim,\n",
        "            hidden_size=self.lstm_size,\n",
        "            num_layers=self.num_layers,\n",
        "            dropout=0.2,\n",
        "        )\n",
        "        self.fc = nn.Linear(self.lstm_size, n_vocab)\n",
        "\n",
        "    def forward(self, x, prev_state):\n",
        "        embed = self.embedding(x)\n",
        "        output, state = self.lstm(embed, prev_state)\n",
        "        logits = self.fc(output)\n",
        "        return logits, state\n",
        "\n",
        "    def init_state(self, sequence_length):\n",
        "        return (torch.zeros(self.num_layers, sequence_length, self.lstm_size).to(self.device),\n",
        "                torch.zeros(self.num_layers, sequence_length, self.lstm_size).to(self.device))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model = LSTMModel(pooh_dataset, device)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##################\n",
        "# modified model #\n",
        "##################\n",
        "class LSTMModel_modified(nn.Module):\n",
        "    def __init__(self, dataset, device):\n",
        "        super(LSTMModel_modified, self).__init__()\n",
        "        self.lstm_size = 1024\n",
        "        self.embedding_dim = 256\n",
        "        self.num_layers = 3\n",
        "        self.device = device\n",
        "\n",
        "\n",
        "        n_vocab = len(dataset.uniq_words)\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=n_vocab,\n",
        "            embedding_dim=self.embedding_dim,\n",
        "        )\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=self.embedding_dim,\n",
        "            hidden_size=self.lstm_size,\n",
        "            num_layers=self.num_layers,\n",
        "            dropout=0.3,\n",
        "        )\n",
        "        self.fc = nn.Linear(self.lstm_size, n_vocab)\n",
        "\n",
        "    def forward(self, x, prev_state):\n",
        "        embed = self.embedding(x)\n",
        "        output, state = self.lstm(embed, prev_state)\n",
        "        logits = self.fc(output)\n",
        "        return logits, state\n",
        "\n",
        "    def init_state(self, sequence_length):\n",
        "        return (torch.zeros(self.num_layers, sequence_length, self.lstm_size).to(self.device),\n",
        "                torch.zeros(self.num_layers, sequence_length, self.lstm_size).to(self.device))\n",
        "\n",
        "model_modified = LSTMModel_modified(pooh_dataset, device)\n",
        "model_modified.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGTRGcmPhZN5",
        "outputId": "67950ee4-c933-4314-df11-293e64ddb61d"
      },
      "id": "lGTRGcmPhZN5",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMModel_modified(\n",
              "  (embedding): Embedding(2548, 256)\n",
              "  (lstm): LSTM(256, 1024, num_layers=3, dropout=0.3)\n",
              "  (fc): Linear(in_features=1024, out_features=2548, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "074d5f7c",
      "metadata": {
        "id": "074d5f7c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 512\n",
        "max_epochs = 30\n",
        "\n",
        "def train(dataset, model):\n",
        "    model.train()\n",
        "\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    for epoch in range(max_epochs):\n",
        "        state_h, state_c = model.init_state(SEQUENCE_LENGTH)\n",
        "\n",
        "        for batch, (x, y) in enumerate(dataloader):\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
        "            loss = criterion(y_pred.transpose(1, 2), y)\n",
        "\n",
        "            state_h = state_h.detach()\n",
        "            state_c = state_c.detach()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print({ 'epoch': epoch, 'batch': batch, 'loss': loss.item() })"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train(pooh_dataset, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jf77WrGq1Tp2",
        "outputId": "fd60b59a-658f-4793-9eda-093545cf19af"
      },
      "id": "jf77WrGq1Tp2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'epoch': 0, 'batch': 113, 'loss': 5.4725470542907715}\n",
            "{'epoch': 1, 'batch': 113, 'loss': 4.953440189361572}\n",
            "{'epoch': 2, 'batch': 113, 'loss': 4.575056552886963}\n",
            "{'epoch': 3, 'batch': 113, 'loss': 4.3051228523254395}\n",
            "{'epoch': 4, 'batch': 113, 'loss': 4.126413822174072}\n",
            "{'epoch': 5, 'batch': 113, 'loss': 3.985083818435669}\n",
            "{'epoch': 6, 'batch': 113, 'loss': 3.8624050617218018}\n",
            "{'epoch': 7, 'batch': 113, 'loss': 3.7447173595428467}\n",
            "{'epoch': 8, 'batch': 113, 'loss': 3.6179933547973633}\n",
            "{'epoch': 9, 'batch': 113, 'loss': 3.500767946243286}\n",
            "{'epoch': 10, 'batch': 113, 'loss': 3.4039530754089355}\n",
            "{'epoch': 11, 'batch': 113, 'loss': 3.3055076599121094}\n",
            "{'epoch': 12, 'batch': 113, 'loss': 3.2000274658203125}\n",
            "{'epoch': 13, 'batch': 113, 'loss': 3.0971553325653076}\n",
            "{'epoch': 14, 'batch': 113, 'loss': 2.997443914413452}\n",
            "{'epoch': 15, 'batch': 113, 'loss': 2.8966305255889893}\n",
            "{'epoch': 16, 'batch': 113, 'loss': 2.8314549922943115}\n",
            "{'epoch': 17, 'batch': 113, 'loss': 2.812638521194458}\n",
            "{'epoch': 18, 'batch': 113, 'loss': 2.6827640533447266}\n",
            "{'epoch': 19, 'batch': 113, 'loss': 2.5885767936706543}\n",
            "{'epoch': 20, 'batch': 113, 'loss': 2.496314764022827}\n",
            "{'epoch': 21, 'batch': 113, 'loss': 2.4230291843414307}\n",
            "{'epoch': 22, 'batch': 113, 'loss': 2.3601486682891846}\n",
            "{'epoch': 23, 'batch': 113, 'loss': 2.2605206966400146}\n",
            "{'epoch': 24, 'batch': 113, 'loss': 2.2059805393218994}\n",
            "{'epoch': 25, 'batch': 113, 'loss': 2.133565664291382}\n",
            "{'epoch': 26, 'batch': 113, 'loss': 2.033362627029419}\n",
            "{'epoch': 27, 'batch': 113, 'loss': 1.9179564714431763}\n",
            "{'epoch': 28, 'batch': 113, 'loss': 1.8808181285858154}\n",
            "{'epoch': 29, 'batch': 113, 'loss': 1.826925277709961}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'pooh_2x512_30ep.model')\n"
      ],
      "metadata": {
        "id": "eCJBOEy53z1b"
      },
      "id": "eCJBOEy53z1b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train modified model\n",
        "train(pooh_dataset, model_modified)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxUC4Tf3syE3",
        "outputId": "2ba4c2ac-6583-4e9b-bc2c-76572a8c1d35"
      },
      "id": "HxUC4Tf3syE3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'epoch': 0, 'batch': 113, 'loss': 5.571034908294678}\n",
            "{'epoch': 1, 'batch': 113, 'loss': 5.493565559387207}\n",
            "{'epoch': 2, 'batch': 113, 'loss': 5.4724555015563965}\n",
            "{'epoch': 3, 'batch': 113, 'loss': 5.46691370010376}\n",
            "{'epoch': 4, 'batch': 113, 'loss': 5.460814952850342}\n",
            "{'epoch': 5, 'batch': 113, 'loss': 5.457488536834717}\n",
            "{'epoch': 6, 'batch': 113, 'loss': 5.451207637786865}\n",
            "{'epoch': 7, 'batch': 113, 'loss': 5.44795560836792}\n",
            "{'epoch': 8, 'batch': 113, 'loss': 5.444857120513916}\n",
            "{'epoch': 9, 'batch': 113, 'loss': 5.442676544189453}\n",
            "{'epoch': 10, 'batch': 113, 'loss': 5.4477739334106445}\n",
            "{'epoch': 11, 'batch': 113, 'loss': 5.440235137939453}\n",
            "{'epoch': 12, 'batch': 113, 'loss': 5.444283485412598}\n",
            "{'epoch': 13, 'batch': 113, 'loss': 5.436881065368652}\n",
            "{'epoch': 14, 'batch': 113, 'loss': 5.433938026428223}\n",
            "{'epoch': 15, 'batch': 113, 'loss': 5.461470127105713}\n",
            "{'epoch': 16, 'batch': 113, 'loss': 5.450287818908691}\n",
            "{'epoch': 17, 'batch': 113, 'loss': 5.479591369628906}\n",
            "{'epoch': 18, 'batch': 113, 'loss': 5.510045528411865}\n",
            "{'epoch': 19, 'batch': 113, 'loss': 5.510703086853027}\n",
            "{'epoch': 20, 'batch': 113, 'loss': 5.659054279327393}\n",
            "{'epoch': 21, 'batch': 113, 'loss': 5.529750823974609}\n",
            "{'epoch': 22, 'batch': 113, 'loss': 5.5126519203186035}\n",
            "{'epoch': 23, 'batch': 113, 'loss': 5.617824554443359}\n",
            "{'epoch': 24, 'batch': 113, 'loss': 5.567748546600342}\n",
            "{'epoch': 25, 'batch': 113, 'loss': 5.677873134613037}\n",
            "{'epoch': 26, 'batch': 113, 'loss': 5.651984214782715}\n",
            "{'epoch': 27, 'batch': 113, 'loss': 5.54794979095459}\n",
            "{'epoch': 28, 'batch': 113, 'loss': 5.568378448486328}\n",
            "{'epoch': 29, 'batch': 113, 'loss': 5.535321235656738}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92ad0a59",
      "metadata": {
        "id": "92ad0a59"
      },
      "outputs": [],
      "source": [
        "torch.save(model_modified.state_dict(), 'pooh_modified.model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38520dec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38520dec",
        "outputId": "081af8a7-0a21-40e4-a640-a2a9cc780224"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "in the morning pooh upon spudge -- forward me certain of did the feeling animals jump it why pooh the . we 2 . once it wisely and see the i then . as wonder 's a understand , time with 's and the , bridge blown was and as what owl you oaktree\n",
            "\n",
            "in the morning piglet her a. hand her come hand to a. hand a. hand her hand hand a. come her hand milne a. a. hand her hand come hand her a. a. her a. to all a. come a. milne a. hand her hand a. we a. jump hand we hand hand her\n",
            "\n",
            "in the morning christopher robin now said . ' '' slept `` made as i thoughtful . , said tigger good i . , a a , a and , his does now four the had the piglet accident head across too thought sorrowfully which . rabbit rabbit the practise grass so be '' ,\n",
            "\n",
            "in the morning rabbit anyone pooh 's -- . `` a pooh 's , looked '' '' could piglet a were because be owl better all to him got at all had back about being shall i . sort '' piglet live ) tigger quickly nervously i , said that gone and it might\n",
            "\n",
            "in the morning owl yours . or strange , was think piglet to being said with sun oh from ? trotted . haycorns this rabbit , was 'll all and been a bees having the played '' he they , and you , important '' them nervously room of river n't i , piglet\n",
            "\n",
            "in the morning tigger of he into let my and for you rabbit '' that why been this means drop , down contradiction `` to `` the got say know , henry had eat round any pooh are among finished to `` thought a ? '' at pooh and '' was , `` in\n",
            "\n",
            "in the morning eeyore you is pooh that pom . many he the as and at eleven the and and not back , . he for one ready how of piglet pooh all at in gloomy . it and on said watched had after `` pooh ! did rum-tum-tum-tiddle-um that `` piglet the ''\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# The predict function is a text generator. You have to modify this code!\n",
        "import random\n",
        "def predict(dataset, model, text, next_words=15):\n",
        "    model.eval()\n",
        "\n",
        "    words = text.split()\n",
        "    state_h, state_c = model.init_state(len(words))\n",
        "\n",
        "    for i in range(0, next_words):\n",
        "        x = torch.tensor([[dataset.word_to_index[w] for w in words[i:]]])\n",
        "        x = x.to(device)\n",
        "\n",
        "        y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
        "\n",
        "        last_word_logits = y_pred[0][-1]\n",
        "        p = torch.nn.functional.softmax(last_word_logits, dim=0).detach().cpu().numpy()\n",
        "        word_index = np.random.choice(len(last_word_logits), p=p)\n",
        "        words.append(dataset.index_to_word[word_index])\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "# DEMO\n",
        "speakers = ['pooh', 'piglet', 'christopher robin', 'rabbit', 'owl', 'tigger', 'eeyore']\n",
        "for s in speakers:\n",
        "    prompt = 'in the morning ' + s\n",
        "    for i in range(1):\n",
        "        print (predict(pooh_dataset, model_modified, prompt, 50))\n",
        "    print ()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea1eb733",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea1eb733",
        "outputId": "5f93310e-d3c9-4b45-b5b3-f68607e13ef6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "863\n"
          ]
        }
      ],
      "source": [
        "# You can use the code if you want\n",
        "\n",
        "from collections import defaultdict as dd\n",
        "\n",
        "vowels = set(\"aoiuye'\")\n",
        "def devowelize(s):\n",
        "    rv = ''.join(a for a in s if a not in vowels)\n",
        "    if rv:\n",
        "        return rv\n",
        "    return '_' # Symbol for words without consonants\n",
        "\n",
        "pooh_words = set(open('pooh_words.txt').read().split())\n",
        "representation = dd(set)\n",
        "\n",
        "for w in pooh_words:\n",
        "    r = devowelize(w)\n",
        "    representation[r].add(w)\n",
        "\n",
        "hard_words = set()\n",
        "for r, ws in representation.items():\n",
        "    if len(ws) > 1:\n",
        "        hard_words.update(ws)\n",
        "\n",
        "print (len(hard_words))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def reconstruct_sentence(model, sentence, dataset, representation, temperature=1.0):\n",
        "    words = sentence\n",
        "    devowelized_sentence = [devowelize(w) for w in words]\n",
        "    model.eval()\n",
        "\n",
        "    state_h, state_c = model.init_state(1)\n",
        "\n",
        "    reconstructed = []\n",
        "    probabilities = []\n",
        "\n",
        "    matching = representation[devowelized_sentence[0]]\n",
        "    reconstructed.append(random.choice(list(matching)))\n",
        "\n",
        "    for i in range(len(devowelized_sentence) - 1):\n",
        "        try:\n",
        "            x = torch.tensor([[dataset.word_to_index[reconstructed[-1]]]])\n",
        "        except KeyError:\n",
        "            pass\n",
        "\n",
        "        x = x.to(device)\n",
        "        y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
        "        last_word_logits = y_pred[0][-1]\n",
        "\n",
        "        matching = representation[devowelized_sentence[i + 1]]\n",
        "        try:\n",
        "            matching_idx = [dataset.word_to_index[match] for match in matching]\n",
        "        except KeyError:\n",
        "            reconstructed.append(random.choice(list(matching)))\n",
        "            continue\n",
        "        p = torch.nn.functional.softmax(last_word_logits/temperature, dim=0).detach().cpu().numpy()\n",
        "        p[~np.isin(np.arange(len(p)), matching_idx)] = 0\n",
        "        p = p/p.sum()\n",
        "        word_index = np.random.choice(len(last_word_logits), p=p)\n",
        "        reconstructed.append(dataset.index_to_word[word_index])\n",
        "        probabilities.append(p[word_index])\n",
        "    return reconstructed, probabilities\n"
      ],
      "metadata": {
        "id": "Lk639DIqGPpl"
      },
      "id": "Lk639DIqGPpl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(original_sequence, reconstructed_sequence):\n",
        "    sa = original_sequence\n",
        "    sb = reconstructed_sequence\n",
        "    score = len([1 for (a,b) in zip(sa, sb) if a == b])\n",
        "    return score / len(original_sequence)"
      ],
      "metadata": {
        "id": "8S-VRmafV7zE"
      },
      "id": "8S-VRmafV7zE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def calculate_likelihood(words, model, dataset):\n",
        "#     model.eval()\n",
        "\n",
        "#     state_h, state_c = model.init_state(len(words))\n",
        "\n",
        "#     x = torch.tensor([[dataset.word_to_index[w] for w in words]])\n",
        "#     x = x.to(device)\n",
        "\n",
        "#     y_pred, _ = model(x, (state_h, state_c))\n",
        "\n",
        "#     likelihood = 0\n",
        "\n",
        "#     for i in range(1, len(words)):\n",
        "#         predicted_word_logits = y_pred[0][i-1]\n",
        "#         predicted_word_index = dataset.word_to_index[words[i]]\n",
        "#         likelihood += predicted_word_logits[predicted_word_index]\n",
        "\n",
        "#     return likelihood.item()\n"
      ],
      "metadata": {
        "id": "bxZbz4eV5myU"
      },
      "id": "bxZbz4eV5myU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('pooh_2x512_30ep.model'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4n3ot1uV3Zb",
        "outputId": "36241d76-6950-4721-dd0a-0cf61cb44569"
      },
      "id": "y4n3ot1uV3Zb",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reconstruction_1, prob_1 = reconstruct_sentence(model, test_dataset, pooh_dataset,representation, temperature=1.0)\n"
      ],
      "metadata": {
        "id": "QF-I9cKXjyz6"
      },
      "id": "QF-I9cKXjyz6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('model_1 accuracy:', accuracy(test_dataset, reconstruction_1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyIbbjzcWRfJ",
        "outputId": "1f27af87-d6c2-4cd3-8d0e-0b74e2e529e3"
      },
      "id": "HyIbbjzcWRfJ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1 accuracy: 0.7886209975762215\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_modified.load_state_dict(torch.load('pooh_modified.model'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1U1hXAOzWUEi",
        "outputId": "60221a53-9156-40ce-e3db-bc07f212b15f"
      },
      "id": "1U1hXAOzWUEi",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reconstruction_2, prob_2 = reconstruct_sentence(model_modified, test_dataset, pooh_dataset,representation, temperature=1.0)\n"
      ],
      "metadata": {
        "id": "Bem4RkLUWcGu"
      },
      "id": "Bem4RkLUWcGu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('model_modified accuracy:',accuracy(test_dataset, reconstruction_2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRySEb7fWgrQ",
        "outputId": "574acde9-eada-4149-f633-e077bb68cc11"
      },
      "id": "LRySEb7fWgrQ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_modified accuracy: 0.7085087383594846\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "273881ae",
      "metadata": {
        "id": "273881ae"
      },
      "source": [
        "You can assume that only words from pooh_words.txt can occur in the reconstructed text. For decoding you have two options (choose one, or implement both ang get **+1** bonus point)\n",
        "\n",
        "1. Sample reconstructed text several times (with quite a low temperature), choose the most likely result.\n",
        "2. Perform beam search.\n",
        "\n",
        "Of course in the sampling procedure you should consider only words matching the given consonants.\n",
        "\n",
        "Report accuracy of your methods (for both language models). The accuracy should be computed by the following function, it should be *greater than 0.25*.\n",
        "\n",
        "\n",
        "```python\n",
        "def accuracy(original_sequence, reconstructed_sequence):\n",
        "    sa = original_sequence\n",
        "    sb = reconstructed_sequence\n",
        "    score = len([1 for (a,b) in zip(sa, sb) if a == b])\n",
        "    return score / len(original_sequence)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a158dfd",
      "metadata": {
        "id": "9a158dfd"
      },
      "source": [
        "## Task 2 (6 points)\n",
        "\n",
        "This task is about text generation. You have to:\n",
        "\n",
        "**A**. Create text corpora containing texts with similar vocabulary (for instance books from the same genre, or written by the same author). This corpora should have approximately 1M words. You can consider using the following sources: Project Gutenberg (https://www.gutenberg.org/), Wolne Lektury (https://wolnelektury.pl/), parts of BookCorpus, https://github.com/soskek/bookcorpus, but generally feel free. Texts could be in English, Polish or any other language you know.\n",
        "\n",
        "**B**. choose the tokenization procedure. It should have two stages:\n",
        "\n",
        "1. word tokenization (you can use nltk.tokenize.word_tokenize, tokenizer from spaCy, pytorch, keras, ...). Test your tokenizer on your corpora, and look at a set of tokens containing both letters and special characters. If some of them should be in your opinion treated as a sequence of tokens, then modify the tokenization procedure\n",
        "\n",
        "2. sub-word tokenization (you can either use the existing procedure, like wordpiece or sentencepiece, or create something by yourself). Here is a simple idea: take 8K most popular words (W), 1K most popular suffixes (S), and 1K most popular prefixes (P). Words in W are its own tokens. Word x outside W should be tokenized as 'p_ _s' where p is the longest prefix of x in P, and s is the longest prefix of W\n",
        "\n",
        "**C**. write text generation procedure. The procedure should fulfill the following requirements:\n",
        "\n",
        "1. it should use the RNN language model (trained on sub-word tokens)\n",
        "2. generated tokens should be presented as a text containing words (without extra spaces, or other extra characters, as begin-of-word introduced during tokenization)\n",
        "3. all words in a generated text should belond to the corpora (note that this is not guaranteed by LSTM)\n",
        "4. in generation Top-P sampling should be used (see NN-NLP.6, slide X)\n",
        "5. in generated texts every token 3-gram should be uniq\n",
        "6. *(optionally, +1 point)* all token bigrams in generated texts occur in the corpora\n",
        "\n",
        "Of course to fulfill these constraints you have to do rejection sampling, or beam search, or ... If you want to be more up-to-date you can also use transformer-like language model. In this case consider using nanoGPT (by A. Karpathy)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install tokenizers -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfGwfQ6gj7sE",
        "outputId": "f3da77bb-fdc2-49a7-bd73-3301ad5ccc02"
      },
      "id": "zfGwfQ6gj7sE",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/7.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/7.8 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/7.8 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from nltk.util import ngrams\n",
        "from nltk import ngrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "from tokenizers import BertWordPieceTokenizer\n"
      ],
      "metadata": {
        "id": "9pLG-0aCY3yz"
      },
      "id": "9pLG-0aCY3yz",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNbnIPlfbOAy",
        "outputId": "2897737b-392c-4635-e3ae-0a27c4e61ac7"
      },
      "id": "wNbnIPlfbOAy",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! gdown https://drive.google.com/uc?id=1G5AAnKiT7H1uTwwCcCrZR6qvVShdFdKD\n",
        "! gdown https://drive.google.com/uc?id=1RifeZWeBiNz3SfL_YTYkGEQLJupOkOd6\n",
        "! ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvrKOrLZe5cz",
        "outputId": "79110485-d6d1-4979-ed44-ce164c17fc8f"
      },
      "id": "dvrKOrLZe5cz",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1G5AAnKiT7H1uTwwCcCrZR6qvVShdFdKD\n",
            "To: /content/Pride and prejudice.txt\n",
            "100% 756k/756k [00:00<00:00, 125MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1RifeZWeBiNz3SfL_YTYkGEQLJupOkOd6\n",
            "To: /content/the three body problem.txt\n",
            "100% 2.71M/2.71M [00:00<00:00, 157MB/s]\n",
            "'Pride and prejudice.txt'   sample_data  'the three body problem.txt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "qO2GZu9MYGeI"
      },
      "id": "qO2GZu9MYGeI",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(path):\n",
        "    with open(path, 'r') as f:\n",
        "        data = f.read()\n",
        "    tokenized_data = word_tokenize(data)\n",
        "    tokenized_data = [x.lower() for x in tokenized_data]\n",
        "    tokenized_data = [re.sub('[^A-Za-z0-9]+', '', x) for x in tokenized_data]\n",
        "    # tokenized_data = [re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*]+\", '', x) for x in tokenized_data]\n",
        "    tokenized_data = [w for w in tokenized_data if len(w) > 0]\n",
        "    return tokenized_data\n",
        "\n",
        "def sub_word_tokenize(data):\n",
        "    tokenizer = BertWordPieceTokenizer()\n",
        "    tokenizer.train_from_iterator(data, vocab_size=10000)\n",
        "    tokenized_data = tokenizer.encode(\" \".join(data))\n",
        "\n",
        "    return tokenizer, tokenized_data"
      ],
      "metadata": {
        "id": "adIe1smGYmsK"
      },
      "id": "adIe1smGYmsK",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_ids, sequence_length, device):\n",
        "\n",
        "        self.words_indexes = data_ids\n",
        "        self.sequence_length = sequence_length\n",
        "        self.device = device\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.words_indexes) - self.sequence_length\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return (\n",
        "            torch.tensor(self.words_indexes[index:index+self.sequence_length], device=self.device),\n",
        "            torch.tensor(self.words_indexes[index+1:index+self.sequence_length+1], device=self.device)\n",
        "        )"
      ],
      "metadata": {
        "id": "85FTaXRebjiw"
      },
      "id": "85FTaXRebjiw",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn, optim\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, n_vocab, device):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.lstm_size = 512\n",
        "        self.embedding_dim = 100\n",
        "        self.num_layers = 2\n",
        "        self.device = device\n",
        "\n",
        "        self.embedding = nn.Embedding(n_vocab, self.embedding_dim)\n",
        "        self.lstm = nn.LSTM(self.embedding_dim, self.lstm_size, self.num_layers, dropout=0.2)\n",
        "        self.fc = nn.Linear(self.lstm_size, n_vocab)\n",
        "\n",
        "    def forward(self, x, prev_state):\n",
        "        embed = self.embedding(x)\n",
        "        output, state = self.lstm(embed, prev_state)\n",
        "        logits = self.fc(output)\n",
        "        return logits, state\n",
        "\n",
        "    def init_state(self, sequence_length):\n",
        "        num_directions = 2 if self.lstm.bidirectional else 1\n",
        "        hidden = torch.zeros(self.num_layers * num_directions, sequence_length, self.lstm_size).to(self.device)\n",
        "        cell = torch.zeros(self.num_layers * num_directions, sequence_length, self.lstm_size).to(self.device)\n",
        "        return hidden, cell\n"
      ],
      "metadata": {
        "id": "UjDBBLR4bkSK"
      },
      "id": "UjDBBLR4bkSK",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = preprocess('Pride and prejudice.txt')\n",
        "# data = preprocess('the three body problem.txt')\n",
        "SEQUENCE_LENGTH = 15\n",
        "tokenizer, tokenized_data = sub_word_tokenize(data)\n",
        "dataset = Dataset(tokenized_data.ids, SEQUENCE_LENGTH, device)\n"
      ],
      "metadata": {
        "id": "MMh40myDbE5Q"
      },
      "id": "MMh40myDbE5Q",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LSTMModel(10000, device)\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lQFnbd8boEb",
        "outputId": "d30d5075-5616-4c81-b81d-eb09baee093c"
      },
      "id": "2lQFnbd8boEb",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMModel(\n",
              "  (embedding): Embedding(10000, 100)\n",
              "  (lstm): LSTM(100, 512, num_layers=2, dropout=0.2)\n",
              "  (fc): Linear(in_features=512, out_features=10000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next(model.parameters()).is_cuda"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AeQ0H59ry0B",
        "outputId": "2083d9a6-9060-4d08-d8e0-78f8b3815f88"
      },
      "id": "_AeQ0H59ry0B",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the same trainning code from previous task\n",
        "train(dataset, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "fp0qqaAzckDg",
        "outputId": "78e1fd4e-d01c-4481-d6af-9ccb72e98b8b"
      },
      "id": "fp0qqaAzckDg",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-e4e0c26fcede>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# the same trainning code from previous task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-b4faf2bdfa04>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset, model)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mstate_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_c\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.save(model.state_dict(), 'three_body.model')\n",
        "torch.save(model.state_dict(), 'Pride and prejudice.model')"
      ],
      "metadata": {
        "id": "epYNpIq-l2wg"
      },
      "id": "epYNpIq-l2wg",
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prefixes_sufixes = {}\n",
        "sufixes_ids = []\n",
        "\n",
        "for word in data:\n",
        "    encoded_word = tokenizer.encode(word)\n",
        "    if len(encoded_word.ids) > 1:\n",
        "        prefixes_sufixes[encoded_word.ids[0]] = encoded_word.ids[1:]\n",
        "\n",
        "for i in range(10000):\n",
        "    token = tokenizer.id_to_token(i)\n",
        "    if token is not None and (token.startswith(\"##\") or token.endswith(\"##\")):\n",
        "        sufixes_ids.append(i)\n"
      ],
      "metadata": {
        "id": "1ND-x6HymKWa"
      },
      "id": "1ND-x6HymKWa",
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def top_p_sampling(p, top_p):\n",
        "    sorted_logits, sorted_indices = torch.sort(torch.from_numpy(p), descending=True)\n",
        "    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "    sorted_indices_to_remove = cumulative_probs > top_p\n",
        "    p[sorted_indices[sorted_indices_to_remove]] = 0.0\n",
        "    p = p/p.sum()\n",
        "    return p"
      ],
      "metadata": {
        "id": "wWGV8TqJnHi0"
      },
      "id": "wWGV8TqJnHi0",
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, tokenizer, text, prefixes_sufixes, next_words=15):\n",
        "    model.eval()\n",
        "\n",
        "    words = tokenizer.encode(text).ids\n",
        "    state_h, state_c = model.init_state(len(words))\n",
        "\n",
        "    continuation = []\n",
        "    existing_3_grams = set(ngrams(words, 3))\n",
        "    for i in range(next_words):\n",
        "        x = torch.tensor(words[i:], device=device)\n",
        "        x = x.unsqueeze(0)\n",
        "        x = x.to(device)\n",
        "\n",
        "        y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
        "        next_from_continuation = -1\n",
        "        if tokenizer.decode([words[-1]]) not in data and len(continuation) > 0:\n",
        "            words.append(continuation.pop())\n",
        "        else:\n",
        "            last_word_logits = y_pred[0][-1]\n",
        "            p = F.softmax(last_word_logits, dim=0).detach().cpu().numpy()\n",
        "\n",
        "            to_exclude = set(sufixes_ids)\n",
        "            if len(continuation) > 0:\n",
        "                next_from_continuation = continuation.pop(0)\n",
        "                to_exclude = to_exclude - {next_from_continuation}\n",
        "\n",
        "            to_exclude = list(to_exclude)\n",
        "            current_2gram = words[2:]\n",
        "            for ngram in existing_3_grams:\n",
        "                if ngram[0] == current_2gram[0] and ngram[1] == current_2gram[1]:\n",
        "                    to_exclude.append(ngram[2])\n",
        "\n",
        "            p[np.isin(np.arange(len(p)), to_exclude)] = 0\n",
        "            p = p / p.sum()\n",
        "            p = top_p_sampling(p, 0.9)\n",
        "            word_index = np.random.choice(len(last_word_logits), p=p)\n",
        "            words.append(word_index)\n",
        "            if not word_index == next_from_continuation:\n",
        "                continuation = []\n",
        "            if word_index in prefixes_sufixes:\n",
        "                continuation = prefixes_sufixes[word_index]\n",
        "        existing_3_grams.add(tuple(words[3:]))\n",
        "    if tokenizer.decode([words[-1]]) not in data and len(continuation) > 0:\n",
        "        words.extend(continuation)\n",
        "    return words, tokenizer.decode(words)\n"
      ],
      "metadata": {
        "id": "uO4kJtH0nJ7X"
      },
      "id": "uO4kJtH0nJ7X",
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt = \"如果我是你，你会怎么做\"\n",
        "prompt = \"it is not easy to say\"\n",
        "words, text = generate_text(model, tokenizer, prompt,prefixes_sufixes, 25)\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmuZKKL_nYSm",
        "outputId": "7ff3361b-5399-465b-8af8-1162481dbc52"
      },
      "id": "KmuZKKL_nYSm",
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "如 果 是 我 你 会 这 么 做 破 ： 到 后 好 完 不 见 多 不 了 那 些 男 aa 去 向 送 发 星 恒 星 环\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt = \"黑暗森林宇宙\"\n",
        "prompt = \"Oh, Jane, had we been less secret\"\n",
        "words, text = generate_text(model, tokenizer, prompt,prefixes_sufixes, 25)\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgPjkWeppWgr",
        "outputId": "7ef68a6d-7643-4ad1-9aeb-8fdbcb6c866b"
      },
      "id": "RgPjkWeppWgr",
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "黑 暗 森 林 宇 宙 云 云 开 切 一 所 位 单 文 与 二 向 三 体 行 不 同 远 不 明 白 向 二 ice 接 触 者 投 放 的\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41e84582",
      "metadata": {
        "id": "41e84582"
      },
      "source": [
        "## Task 3 (4 or 6 p)\n",
        "\n",
        "In this task you have to create a network which looks at characters of the word and tries to guess whether the word is a noun, a verb, an adjective, and so on. To be more precise: the input is a word (without context), the output is a POS-tag (Part-of-Speech). Since some words are unambiguous, and we have no context, our network is supposed to return the set of possible tags.\n",
        "\n",
        "The data is taken from Universal Dependencies English corpus, and of course it contains errors, especially because not all possible tags occured in the data.\n",
        "\n",
        "Train a network (4p) or two networks (+2p) solving this task. Both networks should look at character n-grams occuring in the word. There are two options:\n",
        "\n",
        "* **Fixed size:** for instance take 2,3, and 4-character suffixes of the word, use them as  features (whith 1-hot encoding). You can also combine prefix and suffix features. Simple, useful trick: when looking at suffixes, add some '_' characters at the beginning of the word to guarantee that shorter words have suffixes of a desired length.\n",
        "\n",
        "* **Variable size:** take for instance 4-grams (or 4 grams and 3-grams), use Deep Averaging Network. Simple trick: add extra character at the beginning and at the end of the word, to add the information, that ngram occurs at special position ('ed' at the end has slightly different meaning that 'ed' in the middle)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pytorch-lightning -q\n",
        "! pip install pip install Lightning -q"
      ],
      "metadata": {
        "id": "V19fhuM3LTLH"
      },
      "id": "V19fhuM3LTLH",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from sklearn import preprocessing\n",
        "import itertools\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import Trainer\n",
        "from lightning.pytorch.loggers import TensorBoardLogger\n"
      ],
      "metadata": {
        "id": "zPy0rUkXryEt"
      },
      "id": "zPy0rUkXryEt",
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! gdown https://drive.google.com/uc?id=189ZMDgDCqnbd35QzXTNwam3pwPQR7_Xt\n",
        "! gdown https://drive.google.com/uc?id=1iFYnnN5z2VAgoPj0SLYjYOKMtgArgNRh\n",
        "! ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5sHxpSd-8Vr",
        "outputId": "ce0738b2-9126-4fb0-9671-6dfc2d9c3cb3"
      },
      "id": "G5sHxpSd-8Vr",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=189ZMDgDCqnbd35QzXTNwam3pwPQR7_Xt\n",
            "To: /content/english_tags_dev.txt\n",
            "100% 224k/224k [00:00<00:00, 3.34MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1iFYnnN5z2VAgoPj0SLYjYOKMtgArgNRh\n",
            "To: /content/english_tags_test.txt\n",
            "100% 40.0k/40.0k [00:00<00:00, 91.8MB/s]\n",
            " english_tags_dev.txt\t sample_data\n",
            " english_tags_test.txt\t'view?usp=drive_link'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, path, device):\n",
        "        self.data = open(path).readlines()\n",
        "        self.device = device\n",
        "        self.suffixes = []\n",
        "        self.labels = []\n",
        "\n",
        "        for line in self.data:\n",
        "            splitted = line.split()\n",
        "            word = splitted[0].lower()\n",
        "            label = splitted[1]\n",
        "            if len(word) < 4:\n",
        "                word = word + \"_\" * (4 - len(word))\n",
        "            self.suffixes.append([word[:2], word[:3], word[:4], word[-4:], word[-4:], word[-2:]])\n",
        "            self.labels.append(label)\n",
        "\n",
        "        self.unique_suffixes = list(set(itertools.chain.from_iterable(self.suffixes)))\n",
        "        self.unique_labels = list(set(self.labels))\n",
        "\n",
        "    def encoder(self):\n",
        "        suffixes_le = preprocessing.LabelEncoder()\n",
        "        labels_le = preprocessing.LabelEncoder()\n",
        "\n",
        "        suffixes_le.fit(self.unique_suffixes)\n",
        "        labels_le.fit(self.unique_labels)\n",
        "\n",
        "        return suffixes_le, labels_le\n",
        "\n",
        "    def transform_data(self, suffixes_le, labels_le):\n",
        "        self.encoded_suffixes = [torch.tensor(np.array(suffixes_le.transform(p)), device=self.device) for p in self.suffixes]\n",
        "        self.encoded_labels = [torch.tensor(np.array(labels_le.transform([g])), device=self.device) for g in self.labels]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.suffixes)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        encoded_suffix = F.one_hot(self.encoded_suffixes[index], num_classes=len(self.unique_suffixes)).sum(dim=0).type(torch.FloatTensor)\n",
        "        encoded_label = self.encoded_labels[index]\n",
        "        return encoded_suffix, encoded_label\n"
      ],
      "metadata": {
        "id": "LeJpkMPt_Vyo"
      },
      "id": "LeJpkMPt_Vyo",
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(pl.LightningModule):\n",
        "    def __init__(self, num_features, num_classes):\n",
        "        super().__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(num_features, 4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(4096, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(2048, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, num_classes)\n",
        "        )\n",
        "\n",
        "        self.loss = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        return self.model(inputs)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = optim.Adam(self.model.parameters())\n",
        "        return optimizer\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y = y.view(-1)\n",
        "        outputs = self(x)\n",
        "        loss = self.loss(outputs, y)\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "        _, idx = torch.topk(outputs, k=5)\n",
        "\n",
        "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "        self.log(\"train_accuracy\", torch.mean((preds == y).float()), on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y = y.view(-1)\n",
        "        outputs = self(x)\n",
        "        loss = self.loss(outputs, y)\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "        _, idx = torch.topk(outputs, k=5)\n",
        "\n",
        "        self.log(\"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "        self.log(\"val_accuracy\", torch.mean((preds == y).float()), on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
        "\n",
        "        return loss\n"
      ],
      "metadata": {
        "id": "cYXP0uG9AE0s"
      },
      "id": "cYXP0uG9AE0s",
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PrintCallback(pl.Callback):\n",
        "    def on_epoch_end(self, trainer, pl_module):\n",
        "        metrics = trainer.callback_metrics\n",
        "        print(f\"Epoch: {trainer.current_epoch}, \"\n",
        "              f\"Train Loss: {metrics['train_loss']:.4f}\")"
      ],
      "metadata": {
        "id": "Tb4sGJTjXYYL"
      },
      "id": "Tb4sGJTjXYYL",
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://lightning.ai/docs/pytorch/stable/common/lightning_module.html\n",
        "https://saturncloud.io/blog/pytorch-lightning-print-accuracy-and-loss-at-the-end-of-each-epoch/"
      ],
      "metadata": {
        "id": "hpxWo627WxS4"
      },
      "id": "hpxWo627WxS4"
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "G35CYIAbF3wS"
      },
      "id": "G35CYIAbF3wS",
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = Dataset('english_tags_dev.txt', device)\n",
        "val_dataset = Dataset('english_tags_test.txt', device)\n",
        "dataset.unique_suffixes = list(set(dataset.unique_suffixes + val_dataset.unique_suffixes))\n",
        "val_dataset.unique_suffixes = list(set(dataset.unique_suffixes + val_dataset.unique_suffixes))\n",
        "dataset.unique_labels = list(set(dataset.unique_labels + val_dataset.unique_labels))\n",
        "val_dataset.unique_labels = list(set(dataset.unique_labels + val_dataset.unique_labels))\n",
        "suffixes_le, labels_le = dataset.encoder()\n",
        "dataset.transform_data(suffixes_le, labels_le)\n",
        "val_dataset.transform_data(suffixes_le, labels_le)"
      ],
      "metadata": {
        "id": "bJNV0oZRFuKF"
      },
      "id": "bJNV0oZRFuKF",
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
      ],
      "metadata": {
        "id": "MbPc6gx0F_ZY"
      },
      "id": "MbPc6gx0F_ZY",
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_features = len(dataset.unique_suffixes)\n",
        "num_classes = len(dataset.unique_labels)"
      ],
      "metadata": {
        "id": "_dzsJEHkGBOq"
      },
      "id": "_dzsJEHkGBOq",
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net(num_features, num_classes)"
      ],
      "metadata": {
        "id": "W6YCxxt0H2rL"
      },
      "id": "W6YCxxt0H2rL",
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logger = TensorBoardLogger(\"logs\", name=\"my_model\")\n",
        "trainer = Trainer(logger=logger,min_steps=100, max_epochs=15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVkryuPTJZN2",
        "outputId": "58f77d42-aa8d-40dd-838a-90300486a829"
      },
      "id": "KVkryuPTJZN2",
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: GPU available: False, used: False\n",
            "INFO:lightning.pytorch.utilities.rank_zero:GPU available: False, used: False\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n",
            "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.fit(model, dataloader, val_dataloader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283,
          "referenced_widgets": [
            "c246eccd3b45478797c784e0b2379ac2",
            "e461c091f67b44b5b2b719e3f23b1070",
            "721091319a24409dac8704c8f5b1b6cf",
            "64964925f7de481f9fe2c999dd4a2181",
            "bc85dca46e474885bff6f5a34cdf7cc2",
            "5c36fcaec71a4c27b92045325a1e122f",
            "6eb929aa900c480eb255ac7eb82d513b",
            "3f225f4ab0154230a40664d7f5894f3f",
            "8a312fd885bd4e979b5ed0a21521ecff",
            "e7c0745712fa46ec923328d33301458b",
            "4290f9c551e64661b509331b5e8cece6",
            "59771f966dd849daabc04c66ad83c359",
            "a56f38ca635f42ab817ecc92a935b8f6",
            "6eace409b3b148f5a6ccc61cef3292df",
            "05b771ada5d7461bacc25db321dbe140",
            "59078106cbb844b08c1cc50b6b185fd2",
            "2e9dcfbe30784b8ba9cd909ff9a0046e",
            "84819e8d97cf4c4ab7d129ab96637c8c",
            "5596ee35d6c445c99a2c85e16e5d0a7b",
            "cf71dec64d81401a9093216f41accf12",
            "033fc3fb235f4cc2a4b0ff86a50bf18f",
            "633d291eb73e40e4acde38475f52e02f"
          ]
        },
        "id": "fiZV-XbaLaJb",
        "outputId": "71180426-62f9-4c8d-ea3a-ccce2ac8299e"
      },
      "id": "fiZV-XbaLaJb",
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name  | Type             | Params\n",
            "-------------------------------------------\n",
            "0 | model | Sequential       | 77.0 M\n",
            "1 | loss  | CrossEntropyLoss | 0     \n",
            "-------------------------------------------\n",
            "77.0 M    Trainable params\n",
            "0         Non-trainable params\n",
            "77.0 M    Total params\n",
            "308.089   Total estimated model params size (MB)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Sanity Checking: 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c246eccd3b45478797c784e0b2379ac2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training: 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "59771f966dd849daabc04c66ad83c359"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py:52: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
            "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ac85fcb",
      "metadata": {
        "id": "4ac85fcb"
      },
      "source": [
        "## Task 4 (5p)\n",
        "\n",
        "Apply seq2seq model (you can modify the code from this tutorial: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html) to compute grapheme to phoneme conversion for English. Train the model on dev_cmu_dict.txt and test it on test_cmu_dict.txt. Report accuracy of your solution using two metrics:\n",
        "* exact match (how many words are perfectly converted to phonemes)\n",
        "* exact match without stress (how many words are perfectly converted to phonemes when we remove the information about stress)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2da193ae",
      "metadata": {
        "id": "2da193ae"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c2d5e47",
      "metadata": {
        "id": "7c2d5e47"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c246eccd3b45478797c784e0b2379ac2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e461c091f67b44b5b2b719e3f23b1070",
              "IPY_MODEL_721091319a24409dac8704c8f5b1b6cf",
              "IPY_MODEL_64964925f7de481f9fe2c999dd4a2181"
            ],
            "layout": "IPY_MODEL_bc85dca46e474885bff6f5a34cdf7cc2"
          }
        },
        "e461c091f67b44b5b2b719e3f23b1070": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c36fcaec71a4c27b92045325a1e122f",
            "placeholder": "​",
            "style": "IPY_MODEL_6eb929aa900c480eb255ac7eb82d513b",
            "value": "Sanity Checking DataLoader 0: 100%"
          }
        },
        "721091319a24409dac8704c8f5b1b6cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f225f4ab0154230a40664d7f5894f3f",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8a312fd885bd4e979b5ed0a21521ecff",
            "value": 2
          }
        },
        "64964925f7de481f9fe2c999dd4a2181": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e7c0745712fa46ec923328d33301458b",
            "placeholder": "​",
            "style": "IPY_MODEL_4290f9c551e64661b509331b5e8cece6",
            "value": " 2/2 [00:00&lt;00:00,  8.48it/s]"
          }
        },
        "bc85dca46e474885bff6f5a34cdf7cc2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": "100%"
          }
        },
        "5c36fcaec71a4c27b92045325a1e122f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6eb929aa900c480eb255ac7eb82d513b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3f225f4ab0154230a40664d7f5894f3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a312fd885bd4e979b5ed0a21521ecff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e7c0745712fa46ec923328d33301458b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4290f9c551e64661b509331b5e8cece6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "59771f966dd849daabc04c66ad83c359": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a56f38ca635f42ab817ecc92a935b8f6",
              "IPY_MODEL_6eace409b3b148f5a6ccc61cef3292df",
              "IPY_MODEL_05b771ada5d7461bacc25db321dbe140"
            ],
            "layout": "IPY_MODEL_59078106cbb844b08c1cc50b6b185fd2"
          }
        },
        "a56f38ca635f42ab817ecc92a935b8f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e9dcfbe30784b8ba9cd909ff9a0046e",
            "placeholder": "​",
            "style": "IPY_MODEL_84819e8d97cf4c4ab7d129ab96637c8c",
            "value": "Epoch 0:   0%"
          }
        },
        "6eace409b3b148f5a6ccc61cef3292df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5596ee35d6c445c99a2c85e16e5d0a7b",
            "max": 1146,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cf71dec64d81401a9093216f41accf12",
            "value": 3
          }
        },
        "05b771ada5d7461bacc25db321dbe140": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_033fc3fb235f4cc2a4b0ff86a50bf18f",
            "placeholder": "​",
            "style": "IPY_MODEL_633d291eb73e40e4acde38475f52e02f",
            "value": " 3/1146 [00:05&lt;35:52,  1.88s/it, v_num=2, train_loss_step=4.540]"
          }
        },
        "59078106cbb844b08c1cc50b6b185fd2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "2e9dcfbe30784b8ba9cd909ff9a0046e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84819e8d97cf4c4ab7d129ab96637c8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5596ee35d6c445c99a2c85e16e5d0a7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf71dec64d81401a9093216f41accf12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "033fc3fb235f4cc2a4b0ff86a50bf18f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "633d291eb73e40e4acde38475f52e02f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}