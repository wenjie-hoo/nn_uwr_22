{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "faca5110",
      "metadata": {
        "id": "faca5110"
      },
      "source": [
        "# Assigment 5\n",
        "\n",
        "**Submission deadlines**:\n",
        "\n",
        "* last lab before 20.06.2023\n",
        "\n",
        "**Points:** Aim to get 6 (updated value) out of 15+ possible points\n",
        "\n",
        "All needed data files are on Drive: <https://drive.google.com/drive/folders/1uufpGn46Mwv4oBwajIeOj4rvAK96iaS-?usp=sharing> (or will be soon :) )\n",
        "\n",
        "## Task 1 (5 points)\n",
        "\n",
        "Consider the vowel reconstruction task -- i.e. inserting missing vowels (aeuioy) to obtain proper English text. For instance for the input sentence:\n",
        "\n",
        "<pre>\n",
        "h m gd smbd hs stln ll m vwls\n",
        "</pre>\n",
        "\n",
        "the best result is\n",
        "\n",
        "<pre>\n",
        "oh my god somebody has stolen all my vowels\n",
        "</pre>\n",
        "\n",
        "In this task both dev and test data come from the two books about Winnie-the-Pooh. You have to train two RNN Language Models on *pooh-train.txt*. For the first model use the code below, for the second choose different hyperparameters (different dropout, smaller number of units or layers, or just do any modification you want).\n",
        "\n",
        "The code below is based on\n",
        "https://www.kdnuggets.com/2020/07/pytorch-lstm-text-generation-tutorial.html"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! gdown https://drive.google.com/uc?id=1-k8e9OG7NOVk73Kkv4WpqNQKHrVVmVXa\n",
        "! gdown https://drive.google.com/uc?id=1ADNyasf6AEUsmz-163DWHw_rSldfnpta\n",
        "! gdown https://drive.google.com/uc?id=1POiC9I_BjZKBQe-7XkW5CW0z8_6inWtY\n",
        "! ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rE43jLtazcP",
        "outputId": "cdbe8f37-b17d-4a4d-d556-76e20fc0016d"
      },
      "id": "7rE43jLtazcP",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-k8e9OG7NOVk73Kkv4WpqNQKHrVVmVXa\n",
            "To: /content/pooh_train.txt\n",
            "100% 255k/255k [00:00<00:00, 145MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ADNyasf6AEUsmz-163DWHw_rSldfnpta\n",
            "To: /content/pooh_test.txt\n",
            "100% 34.6k/34.6k [00:00<00:00, 115MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1POiC9I_BjZKBQe-7XkW5CW0z8_6inWtY\n",
            "To: /content/pooh_words.txt\n",
            "100% 20.4k/20.4k [00:00<00:00, 72.1MB/s]\n",
            "pooh_test.txt  pooh_train.txt  pooh_words.txt  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "78d50e03",
      "metadata": {
        "id": "78d50e03"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from collections import Counter\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "SEQUENCE_LENGTH = 15\n",
        "\n",
        "class PoohDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, sequence_length, device):\n",
        "        txt = open('pooh_train.txt').read()\n",
        "\n",
        "        self.words = txt.lower().split() # The text is already tokenized\n",
        "\n",
        "        self.uniq_words = self.get_uniq_words()\n",
        "\n",
        "        self.index_to_word = {index: word for index, word in enumerate(self.uniq_words)}\n",
        "        self.word_to_index = {word: index for index, word in enumerate(self.uniq_words)}\n",
        "\n",
        "        self.words_indexes = [self.word_to_index[w] for w in self.words]\n",
        "        self.sequence_length = sequence_length\n",
        "        self.device = device\n",
        "\n",
        "\n",
        "    def get_uniq_words(self):\n",
        "        word_counts = Counter(self.words)\n",
        "        return sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.words_indexes) - self.sequence_length\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return (\n",
        "            torch.tensor(self.words_indexes[index:index+self.sequence_length], device=self.device),\n",
        "            torch.tensor(self.words_indexes[index+1:index+self.sequence_length+1], device=self.device)\n",
        "        )\n",
        "\n",
        "pooh_dataset = PoohDataset(SEQUENCE_LENGTH, device)\n",
        "test_dataset = open('pooh_test.txt').read()\n",
        "test_dataset = test_dataset.lower().split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "2b22a87b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b22a87b",
        "outputId": "5dee7ad3-317f-486e-833a-d4fec9bedf15"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMModel(\n",
              "  (embedding): Embedding(2548, 100)\n",
              "  (lstm): LSTM(100, 512, num_layers=2, dropout=0.2)\n",
              "  (fc): Linear(in_features=512, out_features=2548, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "from torch import nn, optim\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, dataset, device):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.lstm_size = 512\n",
        "        self.embedding_dim = 100\n",
        "        self.num_layers = 2\n",
        "        self.device = device\n",
        "\n",
        "\n",
        "        n_vocab = len(dataset.uniq_words)\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=n_vocab,\n",
        "            embedding_dim=self.embedding_dim,\n",
        "        )\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=self.embedding_dim,\n",
        "            hidden_size=self.lstm_size,\n",
        "            num_layers=self.num_layers,\n",
        "            dropout=0.2,\n",
        "        )\n",
        "        self.fc = nn.Linear(self.lstm_size, n_vocab)\n",
        "\n",
        "    def forward(self, x, prev_state):\n",
        "        embed = self.embedding(x)\n",
        "        output, state = self.lstm(embed, prev_state)\n",
        "        logits = self.fc(output)\n",
        "        return logits, state\n",
        "\n",
        "    def init_state(self, sequence_length):\n",
        "        return (torch.zeros(self.num_layers, sequence_length, self.lstm_size).to(self.device),\n",
        "                torch.zeros(self.num_layers, sequence_length, self.lstm_size).to(self.device))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model = LSTMModel(pooh_dataset, device)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##################\n",
        "# modified model #\n",
        "##################\n",
        "class LSTMModel_modified(nn.Module):\n",
        "    def __init__(self, dataset, device):\n",
        "        super(LSTMModel_modified, self).__init__()\n",
        "        self.lstm_size = 1024\n",
        "        self.embedding_dim = 256\n",
        "        self.num_layers = 3\n",
        "        self.device = device\n",
        "\n",
        "\n",
        "        n_vocab = len(dataset.uniq_words)\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=n_vocab,\n",
        "            embedding_dim=self.embedding_dim,\n",
        "        )\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=self.embedding_dim,\n",
        "            hidden_size=self.lstm_size,\n",
        "            num_layers=self.num_layers,\n",
        "            dropout=0.3,\n",
        "        )\n",
        "        self.fc = nn.Linear(self.lstm_size, n_vocab)\n",
        "\n",
        "    def forward(self, x, prev_state):\n",
        "        embed = self.embedding(x)\n",
        "        output, state = self.lstm(embed, prev_state)\n",
        "        logits = self.fc(output)\n",
        "        return logits, state\n",
        "\n",
        "    def init_state(self, sequence_length):\n",
        "        return (torch.zeros(self.num_layers, sequence_length, self.lstm_size).to(self.device),\n",
        "                torch.zeros(self.num_layers, sequence_length, self.lstm_size).to(self.device))\n",
        "\n",
        "model_modified = LSTMModel_modified(pooh_dataset, device)\n",
        "model_modified.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGTRGcmPhZN5",
        "outputId": "67950ee4-c933-4314-df11-293e64ddb61d"
      },
      "id": "lGTRGcmPhZN5",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMModel_modified(\n",
              "  (embedding): Embedding(2548, 256)\n",
              "  (lstm): LSTM(256, 1024, num_layers=3, dropout=0.3)\n",
              "  (fc): Linear(in_features=1024, out_features=2548, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "074d5f7c",
      "metadata": {
        "id": "074d5f7c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 512\n",
        "max_epochs = 30\n",
        "\n",
        "def train(dataset, model):\n",
        "    model.train()\n",
        "\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    for epoch in range(max_epochs):\n",
        "        state_h, state_c = model.init_state(SEQUENCE_LENGTH)\n",
        "\n",
        "        for batch, (x, y) in enumerate(dataloader):\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
        "            loss = criterion(y_pred.transpose(1, 2), y)\n",
        "\n",
        "            state_h = state_h.detach()\n",
        "            state_c = state_c.detach()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print({ 'epoch': epoch, 'batch': batch, 'loss': loss.item() })"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train(pooh_dataset, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jf77WrGq1Tp2",
        "outputId": "fd60b59a-658f-4793-9eda-093545cf19af"
      },
      "id": "jf77WrGq1Tp2",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'epoch': 0, 'batch': 113, 'loss': 5.4725470542907715}\n",
            "{'epoch': 1, 'batch': 113, 'loss': 4.953440189361572}\n",
            "{'epoch': 2, 'batch': 113, 'loss': 4.575056552886963}\n",
            "{'epoch': 3, 'batch': 113, 'loss': 4.3051228523254395}\n",
            "{'epoch': 4, 'batch': 113, 'loss': 4.126413822174072}\n",
            "{'epoch': 5, 'batch': 113, 'loss': 3.985083818435669}\n",
            "{'epoch': 6, 'batch': 113, 'loss': 3.8624050617218018}\n",
            "{'epoch': 7, 'batch': 113, 'loss': 3.7447173595428467}\n",
            "{'epoch': 8, 'batch': 113, 'loss': 3.6179933547973633}\n",
            "{'epoch': 9, 'batch': 113, 'loss': 3.500767946243286}\n",
            "{'epoch': 10, 'batch': 113, 'loss': 3.4039530754089355}\n",
            "{'epoch': 11, 'batch': 113, 'loss': 3.3055076599121094}\n",
            "{'epoch': 12, 'batch': 113, 'loss': 3.2000274658203125}\n",
            "{'epoch': 13, 'batch': 113, 'loss': 3.0971553325653076}\n",
            "{'epoch': 14, 'batch': 113, 'loss': 2.997443914413452}\n",
            "{'epoch': 15, 'batch': 113, 'loss': 2.8966305255889893}\n",
            "{'epoch': 16, 'batch': 113, 'loss': 2.8314549922943115}\n",
            "{'epoch': 17, 'batch': 113, 'loss': 2.812638521194458}\n",
            "{'epoch': 18, 'batch': 113, 'loss': 2.6827640533447266}\n",
            "{'epoch': 19, 'batch': 113, 'loss': 2.5885767936706543}\n",
            "{'epoch': 20, 'batch': 113, 'loss': 2.496314764022827}\n",
            "{'epoch': 21, 'batch': 113, 'loss': 2.4230291843414307}\n",
            "{'epoch': 22, 'batch': 113, 'loss': 2.3601486682891846}\n",
            "{'epoch': 23, 'batch': 113, 'loss': 2.2605206966400146}\n",
            "{'epoch': 24, 'batch': 113, 'loss': 2.2059805393218994}\n",
            "{'epoch': 25, 'batch': 113, 'loss': 2.133565664291382}\n",
            "{'epoch': 26, 'batch': 113, 'loss': 2.033362627029419}\n",
            "{'epoch': 27, 'batch': 113, 'loss': 1.9179564714431763}\n",
            "{'epoch': 28, 'batch': 113, 'loss': 1.8808181285858154}\n",
            "{'epoch': 29, 'batch': 113, 'loss': 1.826925277709961}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'pooh_2x512_30ep.model')\n"
      ],
      "metadata": {
        "id": "eCJBOEy53z1b"
      },
      "id": "eCJBOEy53z1b",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train modified model\n",
        "train(pooh_dataset, model_modified)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxUC4Tf3syE3",
        "outputId": "2ba4c2ac-6583-4e9b-bc2c-76572a8c1d35"
      },
      "id": "HxUC4Tf3syE3",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'epoch': 0, 'batch': 113, 'loss': 5.571034908294678}\n",
            "{'epoch': 1, 'batch': 113, 'loss': 5.493565559387207}\n",
            "{'epoch': 2, 'batch': 113, 'loss': 5.4724555015563965}\n",
            "{'epoch': 3, 'batch': 113, 'loss': 5.46691370010376}\n",
            "{'epoch': 4, 'batch': 113, 'loss': 5.460814952850342}\n",
            "{'epoch': 5, 'batch': 113, 'loss': 5.457488536834717}\n",
            "{'epoch': 6, 'batch': 113, 'loss': 5.451207637786865}\n",
            "{'epoch': 7, 'batch': 113, 'loss': 5.44795560836792}\n",
            "{'epoch': 8, 'batch': 113, 'loss': 5.444857120513916}\n",
            "{'epoch': 9, 'batch': 113, 'loss': 5.442676544189453}\n",
            "{'epoch': 10, 'batch': 113, 'loss': 5.4477739334106445}\n",
            "{'epoch': 11, 'batch': 113, 'loss': 5.440235137939453}\n",
            "{'epoch': 12, 'batch': 113, 'loss': 5.444283485412598}\n",
            "{'epoch': 13, 'batch': 113, 'loss': 5.436881065368652}\n",
            "{'epoch': 14, 'batch': 113, 'loss': 5.433938026428223}\n",
            "{'epoch': 15, 'batch': 113, 'loss': 5.461470127105713}\n",
            "{'epoch': 16, 'batch': 113, 'loss': 5.450287818908691}\n",
            "{'epoch': 17, 'batch': 113, 'loss': 5.479591369628906}\n",
            "{'epoch': 18, 'batch': 113, 'loss': 5.510045528411865}\n",
            "{'epoch': 19, 'batch': 113, 'loss': 5.510703086853027}\n",
            "{'epoch': 20, 'batch': 113, 'loss': 5.659054279327393}\n",
            "{'epoch': 21, 'batch': 113, 'loss': 5.529750823974609}\n",
            "{'epoch': 22, 'batch': 113, 'loss': 5.5126519203186035}\n",
            "{'epoch': 23, 'batch': 113, 'loss': 5.617824554443359}\n",
            "{'epoch': 24, 'batch': 113, 'loss': 5.567748546600342}\n",
            "{'epoch': 25, 'batch': 113, 'loss': 5.677873134613037}\n",
            "{'epoch': 26, 'batch': 113, 'loss': 5.651984214782715}\n",
            "{'epoch': 27, 'batch': 113, 'loss': 5.54794979095459}\n",
            "{'epoch': 28, 'batch': 113, 'loss': 5.568378448486328}\n",
            "{'epoch': 29, 'batch': 113, 'loss': 5.535321235656738}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "92ad0a59",
      "metadata": {
        "id": "92ad0a59"
      },
      "outputs": [],
      "source": [
        "torch.save(model_modified.state_dict(), 'pooh_modified.model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "38520dec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38520dec",
        "outputId": "081af8a7-0a21-40e4-a640-a2a9cc780224"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "in the morning pooh upon spudge -- forward me certain of did the feeling animals jump it why pooh the . we 2 . once it wisely and see the i then . as wonder 's a understand , time with 's and the , bridge blown was and as what owl you oaktree\n",
            "\n",
            "in the morning piglet her a. hand her come hand to a. hand a. hand her hand hand a. come her hand milne a. a. hand her hand come hand her a. a. her a. to all a. come a. milne a. hand her hand a. we a. jump hand we hand hand her\n",
            "\n",
            "in the morning christopher robin now said . ' '' slept `` made as i thoughtful . , said tigger good i . , a a , a and , his does now four the had the piglet accident head across too thought sorrowfully which . rabbit rabbit the practise grass so be '' ,\n",
            "\n",
            "in the morning rabbit anyone pooh 's -- . `` a pooh 's , looked '' '' could piglet a were because be owl better all to him got at all had back about being shall i . sort '' piglet live ) tigger quickly nervously i , said that gone and it might\n",
            "\n",
            "in the morning owl yours . or strange , was think piglet to being said with sun oh from ? trotted . haycorns this rabbit , was 'll all and been a bees having the played '' he they , and you , important '' them nervously room of river n't i , piglet\n",
            "\n",
            "in the morning tigger of he into let my and for you rabbit '' that why been this means drop , down contradiction `` to `` the got say know , henry had eat round any pooh are among finished to `` thought a ? '' at pooh and '' was , `` in\n",
            "\n",
            "in the morning eeyore you is pooh that pom . many he the as and at eleven the and and not back , . he for one ready how of piglet pooh all at in gloomy . it and on said watched had after `` pooh ! did rum-tum-tum-tiddle-um that `` piglet the ''\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# The predict function is a text generator. You have to modify this code!\n",
        "import random\n",
        "def predict(dataset, model, text, next_words=15):\n",
        "    model.eval()\n",
        "\n",
        "    words = text.split()\n",
        "    state_h, state_c = model.init_state(len(words))\n",
        "\n",
        "    for i in range(0, next_words):\n",
        "        x = torch.tensor([[dataset.word_to_index[w] for w in words[i:]]])\n",
        "        x = x.to(device)\n",
        "\n",
        "        y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
        "\n",
        "        last_word_logits = y_pred[0][-1]\n",
        "        p = torch.nn.functional.softmax(last_word_logits, dim=0).detach().cpu().numpy()\n",
        "        word_index = np.random.choice(len(last_word_logits), p=p)\n",
        "        words.append(dataset.index_to_word[word_index])\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "# DEMO\n",
        "speakers = ['pooh', 'piglet', 'christopher robin', 'rabbit', 'owl', 'tigger', 'eeyore']\n",
        "for s in speakers:\n",
        "    prompt = 'in the morning ' + s\n",
        "    for i in range(1):\n",
        "        print (predict(pooh_dataset, model_modified, prompt, 50))\n",
        "    print ()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "ea1eb733",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea1eb733",
        "outputId": "5f93310e-d3c9-4b45-b5b3-f68607e13ef6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "863\n"
          ]
        }
      ],
      "source": [
        "# You can use the code if you want\n",
        "\n",
        "from collections import defaultdict as dd\n",
        "\n",
        "vowels = set(\"aoiuye'\")\n",
        "def devowelize(s):\n",
        "    rv = ''.join(a for a in s if a not in vowels)\n",
        "    if rv:\n",
        "        return rv\n",
        "    return '_' # Symbol for words without consonants\n",
        "\n",
        "pooh_words = set(open('pooh_words.txt').read().split())\n",
        "representation = dd(set)\n",
        "\n",
        "for w in pooh_words:\n",
        "    r = devowelize(w)\n",
        "    representation[r].add(w)\n",
        "\n",
        "hard_words = set()\n",
        "for r, ws in representation.items():\n",
        "    if len(ws) > 1:\n",
        "        hard_words.update(ws)\n",
        "\n",
        "print (len(hard_words))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def reconstruct_sentence(model, sentence, dataset, representation, temperature=1.0):\n",
        "    words = sentence\n",
        "    devowelized_sentence = [devowelize(w) for w in words]\n",
        "    model.eval()\n",
        "\n",
        "    state_h, state_c = model.init_state(1)\n",
        "\n",
        "    reconstructed = []\n",
        "    probabilities = []\n",
        "\n",
        "    matching = representation[devowelized_sentence[0]]\n",
        "    reconstructed.append(random.choice(list(matching)))\n",
        "\n",
        "    for i in range(len(devowelized_sentence) - 1):\n",
        "        try:\n",
        "            x = torch.tensor([[dataset.word_to_index[reconstructed[-1]]]])\n",
        "        except KeyError:\n",
        "            pass\n",
        "\n",
        "        x = x.to(device)\n",
        "        y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
        "        last_word_logits = y_pred[0][-1]\n",
        "\n",
        "        matching = representation[devowelized_sentence[i + 1]]\n",
        "        try:\n",
        "            matching_idx = [dataset.word_to_index[match] for match in matching]\n",
        "        except KeyError:\n",
        "            reconstructed.append(random.choice(list(matching)))\n",
        "            continue\n",
        "        p = torch.nn.functional.softmax(last_word_logits/temperature, dim=0).detach().cpu().numpy()\n",
        "        p[~np.isin(np.arange(len(p)), matching_idx)] = 0\n",
        "        p = p/p.sum()\n",
        "        word_index = np.random.choice(len(last_word_logits), p=p)\n",
        "        reconstructed.append(dataset.index_to_word[word_index])\n",
        "        probabilities.append(p[word_index])\n",
        "    return reconstructed, probabilities\n"
      ],
      "metadata": {
        "id": "Lk639DIqGPpl"
      },
      "id": "Lk639DIqGPpl",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(original_sequence, reconstructed_sequence):\n",
        "    sa = original_sequence\n",
        "    sb = reconstructed_sequence\n",
        "    score = len([1 for (a,b) in zip(sa, sb) if a == b])\n",
        "    return score / len(original_sequence)"
      ],
      "metadata": {
        "id": "8S-VRmafV7zE"
      },
      "id": "8S-VRmafV7zE",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def calculate_likelihood(words, model, dataset):\n",
        "#     model.eval()\n",
        "\n",
        "#     state_h, state_c = model.init_state(len(words))\n",
        "\n",
        "#     x = torch.tensor([[dataset.word_to_index[w] for w in words]])\n",
        "#     x = x.to(device)\n",
        "\n",
        "#     y_pred, _ = model(x, (state_h, state_c))\n",
        "\n",
        "#     likelihood = 0\n",
        "\n",
        "#     for i in range(1, len(words)):\n",
        "#         predicted_word_logits = y_pred[0][i-1]\n",
        "#         predicted_word_index = dataset.word_to_index[words[i]]\n",
        "#         likelihood += predicted_word_logits[predicted_word_index]\n",
        "\n",
        "#     return likelihood.item()\n"
      ],
      "metadata": {
        "id": "bxZbz4eV5myU"
      },
      "id": "bxZbz4eV5myU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('pooh_2x512_30ep.model'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4n3ot1uV3Zb",
        "outputId": "36241d76-6950-4721-dd0a-0cf61cb44569"
      },
      "id": "y4n3ot1uV3Zb",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reconstruction_1, prob_1 = reconstruct_sentence(model, test_dataset, pooh_dataset,representation, temperature=1.0)\n"
      ],
      "metadata": {
        "id": "QF-I9cKXjyz6"
      },
      "id": "QF-I9cKXjyz6",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('model_1 accuracy:', accuracy(test_dataset, reconstruction_1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyIbbjzcWRfJ",
        "outputId": "1f27af87-d6c2-4cd3-8d0e-0b74e2e529e3"
      },
      "id": "HyIbbjzcWRfJ",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1 accuracy: 0.7886209975762215\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_modified.load_state_dict(torch.load('pooh_modified.model'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1U1hXAOzWUEi",
        "outputId": "60221a53-9156-40ce-e3db-bc07f212b15f"
      },
      "id": "1U1hXAOzWUEi",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reconstruction_2, prob_2 = reconstruct_sentence(model_modified, test_dataset, pooh_dataset,representation, temperature=1.0)\n"
      ],
      "metadata": {
        "id": "Bem4RkLUWcGu"
      },
      "id": "Bem4RkLUWcGu",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('model_modified accuracy:',accuracy(test_dataset, reconstruction_2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRySEb7fWgrQ",
        "outputId": "574acde9-eada-4149-f633-e077bb68cc11"
      },
      "id": "LRySEb7fWgrQ",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_modified accuracy: 0.7085087383594846\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "273881ae",
      "metadata": {
        "id": "273881ae"
      },
      "source": [
        "You can assume that only words from pooh_words.txt can occur in the reconstructed text. For decoding you have two options (choose one, or implement both ang get **+1** bonus point)\n",
        "\n",
        "1. Sample reconstructed text several times (with quite a low temperature), choose the most likely result.\n",
        "2. Perform beam search.\n",
        "\n",
        "Of course in the sampling procedure you should consider only words matching the given consonants.\n",
        "\n",
        "Report accuracy of your methods (for both language models). The accuracy should be computed by the following function, it should be *greater than 0.25*.\n",
        "\n",
        "\n",
        "```python\n",
        "def accuracy(original_sequence, reconstructed_sequence):\n",
        "    sa = original_sequence\n",
        "    sb = reconstructed_sequence\n",
        "    score = len([1 for (a,b) in zip(sa, sb) if a == b])\n",
        "    return score / len(original_sequence)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a158dfd",
      "metadata": {
        "id": "9a158dfd"
      },
      "source": [
        "## Task 2 (6 points)\n",
        "\n",
        "This task is about text generation. You have to:\n",
        "\n",
        "**A**. Create text corpora containing texts with similar vocabulary (for instance books from the same genre, or written by the same author). This corpora should have approximately 1M words. You can consider using the following sources: Project Gutenberg (https://www.gutenberg.org/), Wolne Lektury (https://wolnelektury.pl/), parts of BookCorpus, https://github.com/soskek/bookcorpus, but generally feel free. Texts could be in English, Polish or any other language you know.\n",
        "\n",
        "**B**. choose the tokenization procedure. It should have two stages:\n",
        "\n",
        "1. word tokenization (you can use nltk.tokenize.word_tokenize, tokenizer from spaCy, pytorch, keras, ...). Test your tokenizer on your corpora, and look at a set of tokens containing both letters and special characters. If some of them should be in your opinion treated as a sequence of tokens, then modify the tokenization procedure\n",
        "\n",
        "2. sub-word tokenization (you can either use the existing procedure, like wordpiece or sentencepiece, or create something by yourself). Here is a simple idea: take 8K most popular words (W), 1K most popular suffixes (S), and 1K most popular prefixes (P). Words in W are its own tokens. Word x outside W should be tokenized as 'p_ _s' where p is the longest prefix of x in P, and s is the longest prefix of W\n",
        "\n",
        "**C**. write text generation procedure. The procedure should fulfill the following requirements:\n",
        "\n",
        "1. it should use the RNN language model (trained on sub-word tokens)\n",
        "2. generated tokens should be presented as a text containing words (without extra spaces, or other extra characters, as begin-of-word introduced during tokenization)\n",
        "3. all words in a generated text should belond to the corpora (note that this is not guaranteed by LSTM)\n",
        "4. in generation Top-P sampling should be used (see NN-NLP.6, slide X)\n",
        "5. in generated texts every token 3-gram should be uniq\n",
        "6. *(optionally, +1 point)* all token bigrams in generated texts occur in the corpora\n",
        "\n",
        "Of course to fulfill these constraints you have to do rejection sampling, or beam search, or ... If you want to be more up-to-date you can also use transformer-like language model. In this case consider using nanoGPT (by A. Karpathy)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from nltk.util import ngrams\n",
        "from nltk import ngrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "from tokenizers import BertWordPieceTokenizer\n"
      ],
      "metadata": {
        "id": "9pLG-0aCY3yz"
      },
      "id": "9pLG-0aCY3yz",
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNbnIPlfbOAy",
        "outputId": "c0f0c8fb-9c26-4da7-cf29-81ba7b4ae8c1"
      },
      "id": "wNbnIPlfbOAy",
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! gdown https://drive.google.com/uc?id=1G5AAnKiT7H1uTwwCcCrZR6qvVShdFdKD\n",
        "! ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvrKOrLZe5cz",
        "outputId": "78176f8c-bbe6-4e9d-f9ea-e2ea97ba5deb"
      },
      "id": "dvrKOrLZe5cz",
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1G5AAnKiT7H1uTwwCcCrZR6qvVShdFdKD\n",
            "To: /content/Pride and prejudice.txt\n",
            "\r  0% 0.00/756k [00:00<?, ?B/s]\r100% 756k/756k [00:00<00:00, 103MB/s]\n",
            " aa.txt\t\t\t pooh_train.txt\t\t   'view?usp=drive_link'\n",
            " pooh_2x512_30ep.model\t pooh_words.txt\t\t    word_vectors_wiki.txt\n",
            " pooh_modified.model\t'Pride and prejudice.txt'\n",
            " pooh_test.txt\t\t sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "qO2GZu9MYGeI"
      },
      "id": "qO2GZu9MYGeI",
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(path):\n",
        "    with open(path, 'r') as f:\n",
        "        data = f.read()\n",
        "    tokenized_data = word_tokenize(data)\n",
        "    tokenized_data = [x.lower() for x in tokenized_data]\n",
        "    tokenized_data = [re.sub('[^A-Za-z0-9]+', '', x) for x in tokenized_data]\n",
        "    tokenized_data = [w for w in tokenized_data if len(w) > 0]\n",
        "    return tokenized_data\n",
        "\n",
        "def sub_word_tokenize(data):\n",
        "    tokenizer = BertWordPieceTokenizer()\n",
        "    tokenizer.train_from_iterator(data, vocab_size=8000)\n",
        "    tokenized_data = tokenizer.encode(\" \".join(data))\n",
        "\n",
        "    return tokenizer, tokenized_data"
      ],
      "metadata": {
        "id": "adIe1smGYmsK"
      },
      "id": "adIe1smGYmsK",
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_ids, sequence_length, device):\n",
        "\n",
        "        self.words_indexes = data_ids\n",
        "        self.sequence_length = sequence_length\n",
        "        self.device = device\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.words_indexes) - self.sequence_length\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return (\n",
        "            torch.tensor(self.words_indexes[index:index+self.sequence_length], device=self.device),\n",
        "            torch.tensor(self.words_indexes[index+1:index+self.sequence_length+1], device=self.device)\n",
        "        )"
      ],
      "metadata": {
        "id": "85FTaXRebjiw"
      },
      "id": "85FTaXRebjiw",
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, n_vocab, device):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.lstm_size = 512\n",
        "        self.embedding_dim = 100\n",
        "        self.num_layers = 2\n",
        "        self.device = device\n",
        "\n",
        "        self.embedding = nn.Embedding(n_vocab, self.embedding_dim)\n",
        "        self.lstm = nn.LSTM(self.embedding_dim, self.lstm_size, self.num_layers, dropout=0.2)\n",
        "        self.fc = nn.Linear(self.lstm_size, n_vocab)\n",
        "\n",
        "    def forward(self, x, prev_state):\n",
        "        embed = self.embedding(x)\n",
        "        output, state = self.lstm(embed, prev_state)\n",
        "        logits = self.fc(output)\n",
        "        return logits, state\n",
        "\n",
        "    def init_state(self, sequence_length):\n",
        "        num_directions = 2 if self.lstm.bidirectional else 1\n",
        "        hidden = torch.zeros(self.num_layers * num_directions, sequence_length, self.lstm_size).to(self.device)\n",
        "        cell = torch.zeros(self.num_layers * num_directions, sequence_length, self.lstm_size).to(self.device)\n",
        "        return hidden, cell\n"
      ],
      "metadata": {
        "id": "UjDBBLR4bkSK"
      },
      "id": "UjDBBLR4bkSK",
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = preprocess('Pride and prejudice.txt')\n",
        "tokenizer, tokenized_data = sub_word_tokenize(data)\n",
        "dataset = Dataset(tokenized_data.ids, SEQUENCE_LENGTH, device)\n"
      ],
      "metadata": {
        "id": "MMh40myDbE5Q"
      },
      "id": "MMh40myDbE5Q",
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LSTMModel(8000, device)\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lQFnbd8boEb",
        "outputId": "46823fa5-7bc7-460b-e5bb-942a9b6e11c9"
      },
      "id": "2lQFnbd8boEb",
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMModel(\n",
              "  (embedding): Embedding(8000, 100)\n",
              "  (lstm): LSTM(100, 512, num_layers=2, dropout=0.2)\n",
              "  (fc): Linear(in_features=512, out_features=8000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train(dataset, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fp0qqaAzckDg",
        "outputId": "18649bad-abae-4453-f5ca-3149aed38016"
      },
      "id": "fp0qqaAzckDg",
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'epoch': 0, 'batch': 264, 'loss': 8.067949295043945}\n",
            "{'epoch': 1, 'batch': 264, 'loss': 7.1042256355285645}\n",
            "{'epoch': 2, 'batch': 264, 'loss': 6.575026512145996}\n",
            "{'epoch': 3, 'batch': 264, 'loss': 6.2271318435668945}\n",
            "{'epoch': 4, 'batch': 264, 'loss': 5.597900867462158}\n",
            "{'epoch': 5, 'batch': 264, 'loss': 5.038427829742432}\n",
            "{'epoch': 6, 'batch': 264, 'loss': 4.614715099334717}\n",
            "{'epoch': 7, 'batch': 264, 'loss': 4.110250949859619}\n",
            "{'epoch': 8, 'batch': 264, 'loss': 3.6042678356170654}\n",
            "{'epoch': 9, 'batch': 264, 'loss': 3.0871434211730957}\n",
            "{'epoch': 10, 'batch': 264, 'loss': 2.697675943374634}\n",
            "{'epoch': 11, 'batch': 264, 'loss': 2.281498670578003}\n",
            "{'epoch': 12, 'batch': 264, 'loss': 1.972225546836853}\n",
            "{'epoch': 13, 'batch': 264, 'loss': 1.7238037586212158}\n",
            "{'epoch': 14, 'batch': 264, 'loss': 1.543869137763977}\n",
            "{'epoch': 15, 'batch': 264, 'loss': 1.3699345588684082}\n",
            "{'epoch': 16, 'batch': 264, 'loss': 1.2488223314285278}\n",
            "{'epoch': 17, 'batch': 264, 'loss': 1.0449751615524292}\n",
            "{'epoch': 18, 'batch': 264, 'loss': 0.9658154249191284}\n",
            "{'epoch': 19, 'batch': 264, 'loss': 0.9113202095031738}\n",
            "{'epoch': 20, 'batch': 264, 'loss': 0.7878768444061279}\n",
            "{'epoch': 21, 'batch': 264, 'loss': 0.6644800901412964}\n",
            "{'epoch': 22, 'batch': 264, 'loss': 0.6256316304206848}\n",
            "{'epoch': 23, 'batch': 264, 'loss': 0.5822001099586487}\n",
            "{'epoch': 24, 'batch': 264, 'loss': 0.5788312554359436}\n",
            "{'epoch': 25, 'batch': 264, 'loss': 0.46789249777793884}\n",
            "{'epoch': 26, 'batch': 264, 'loss': 0.4766520857810974}\n",
            "{'epoch': 27, 'batch': 264, 'loss': 0.46584510803222656}\n",
            "{'epoch': 28, 'batch': 264, 'loss': 0.44146591424942017}\n",
            "{'epoch': 29, 'batch': 264, 'loss': 0.4026448428630829}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'Pride_and_prejudice.model')"
      ],
      "metadata": {
        "id": "epYNpIq-l2wg"
      },
      "id": "epYNpIq-l2wg",
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prefixes_sufixes = {}\n",
        "sufixes_ids = []\n",
        "\n",
        "for word in data:\n",
        "    encoded_word = tokenizer.encode(word)\n",
        "    if len(encoded_word.ids) > 1:\n",
        "        prefixes_sufixes[encoded_word.ids[0]] = encoded_word.ids[1:]\n",
        "\n",
        "for i in range(10000):\n",
        "    token = tokenizer.id_to_token(i)\n",
        "    if token is not None and (token.startswith(\"##\") or token.endswith(\"##\")):\n",
        "        sufixes_ids.append(i)\n"
      ],
      "metadata": {
        "id": "1ND-x6HymKWa"
      },
      "id": "1ND-x6HymKWa",
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def top_p_sampling(p, top_p):\n",
        "    sorted_logits, sorted_indices = torch.sort(torch.from_numpy(p), descending=True)\n",
        "    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "    sorted_indices_to_remove = cumulative_probs > top_p\n",
        "    p[sorted_indices[sorted_indices_to_remove]] = 0.0\n",
        "    p = p/p.sum()\n",
        "    return p"
      ],
      "metadata": {
        "id": "wWGV8TqJnHi0"
      },
      "id": "wWGV8TqJnHi0",
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, tokenizer, text, prefixes_sufixes, next_words=15):\n",
        "    model.eval()\n",
        "\n",
        "    words = tokenizer.encode(text).ids\n",
        "    state_h, state_c = model.init_state(len(words))\n",
        "\n",
        "    continuation = []\n",
        "    existing_3_grams = set(ngrams(words, 3))\n",
        "    for i in range(next_words):\n",
        "        x = torch.tensor(words[i:], device=device)\n",
        "        x = x.unsqueeze(0)\n",
        "        x = x.to(device)\n",
        "\n",
        "        y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
        "        next_from_continuation = -1\n",
        "        if tokenizer.decode([words[-1]]) not in data and len(continuation) > 0:\n",
        "            words.append(continuation.pop())\n",
        "        else:\n",
        "            last_word_logits = y_pred[0][-1]\n",
        "            p = F.softmax(last_word_logits, dim=0).detach().cpu().numpy()\n",
        "\n",
        "            to_exclude = set(sufixes_ids)\n",
        "            if len(continuation) > 0:\n",
        "                next_from_continuation = continuation.pop(0)\n",
        "                to_exclude = to_exclude - {next_from_continuation}\n",
        "\n",
        "            to_exclude = list(to_exclude)\n",
        "            current_2gram = words[2:]\n",
        "            for ngram in existing_3_grams:\n",
        "                if ngram[0] == current_2gram[0] and ngram[1] == current_2gram[1]:\n",
        "                    to_exclude.append(ngram[2])\n",
        "\n",
        "            p[np.isin(np.arange(len(p)), to_exclude)] = 0\n",
        "            p = p / p.sum()\n",
        "            p = top_p_sampling(p, 0.9)\n",
        "            word_index = np.random.choice(len(last_word_logits), p=p)\n",
        "            words.append(word_index)\n",
        "            if not word_index == next_from_continuation:\n",
        "                continuation = []\n",
        "            if word_index in prefixes_sufixes:\n",
        "                continuation = prefixes_sufixes[word_index]\n",
        "        existing_3_grams.add(tuple(words[3:]))\n",
        "    if tokenizer.decode([words[-1]]) not in data and len(continuation) > 0:\n",
        "        words.extend(continuation)\n",
        "    return words, tokenizer.decode(words)\n"
      ],
      "metadata": {
        "id": "uO4kJtH0nJ7X"
      },
      "id": "uO4kJtH0nJ7X",
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"it is not easy to say\"\n",
        "words, text = generate_text(model, tokenizer, prompt,prefixes_sufixes, 25)\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmuZKKL_nYSm",
        "outputId": "b3fa9acc-838a-480a-bdee-e2c1426524f7"
      },
      "id": "KmuZKKL_nYSm",
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "it is not easy to say 1e1 repent stated quadrille amendment seeming online revolution commanded studied submit tempt parsonagehouse fill hurt speedily despising lying exposing amaz solicit amaz amaz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Oh, Jane, had we been less secret\"\n",
        "words, text = generate_text(model, tokenizer, prompt,prefixes_sufixes, 25)\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgPjkWeppWgr",
        "outputId": "283de8df-40d2-4e09-d829-ec0f3e17b352"
      },
      "id": "RgPjkWeppWgr",
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "oh jane had we been less secret deranged cool understand me both will you come whether you ought to be in having so much no more rich sir than wishes now\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41e84582",
      "metadata": {
        "id": "41e84582"
      },
      "source": [
        "## Task 3 (4 or 6 p)\n",
        "\n",
        "In this task you have to create a network which looks at characters of the word and tries to guess whether the word is a noun, a verb, an adjective, and so on. To be more precise: the input is a word (without context), the output is a POS-tag (Part-of-Speech). Since some words are unambiguous, and we have no context, our network is supposed to return the set of possible tags.\n",
        "\n",
        "The data is taken from Universal Dependencies English corpus, and of course it contains errors, especially because not all possible tags occured in the data.\n",
        "\n",
        "Train a network (4p) or two networks (+2p) solving this task. Both networks should look at character n-grams occuring in the word. There are two options:\n",
        "\n",
        "* **Fixed size:** for instance take 2,3, and 4-character suffixes of the word, use them as  features (whith 1-hot encoding). You can also combine prefix and suffix features. Simple, useful trick: when looking at suffixes, add some '_' characters at the beginning of the word to guarantee that shorter words have suffixes of a desired length.\n",
        "\n",
        "* **Variable size:** take for instance 4-grams (or 4 grams and 3-grams), use Deep Averaging Network. Simple trick: add extra character at the beginning and at the end of the word, to add the information, that ngram occurs at special position ('ed' at the end has slightly different meaning that 'ed' in the middle)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zPy0rUkXryEt"
      },
      "id": "zPy0rUkXryEt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "4ac85fcb",
      "metadata": {
        "id": "4ac85fcb"
      },
      "source": [
        "## Task 4 (5p)\n",
        "\n",
        "Apply seq2seq model (you can modify the code from this tutorial: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html) to compute grapheme to phoneme conversion for English. Train the model on dev_cmu_dict.txt and test it on test_cmu_dict.txt. Report accuracy of your solution using two metrics:\n",
        "* exact match (how many words are perfectly converted to phonemes)\n",
        "* exact match without stress (how many words are perfectly converted to phonemes when we remove the information about stress)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2da193ae",
      "metadata": {
        "id": "2da193ae"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c2d5e47",
      "metadata": {
        "id": "7c2d5e47"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c538fb76",
      "metadata": {
        "id": "c538fb76"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4feefe2",
      "metadata": {
        "id": "b4feefe2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}